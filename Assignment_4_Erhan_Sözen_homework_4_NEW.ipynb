{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erhansozen/Assignments-LearningPortfolio/blob/main/Erhan_S%C3%B6zen_homework_4_NEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory\n",
        "\n",
        "In the following assignment, your task is to complete the MNIST Basics chapter. It is best to repeat everything from last week and try to answer the following questions. Afterwards you have to summarize the learned facts with two programming tasks.\n",
        "\n",
        "What is \"torch.cat()\" and \".view(-1, 28*28)\" doing in the beginning of the \"The MNIST Loss Function\" chapter?\n",
        "\n",
        "✈ In PyTorch, torch.cat() is a function that concatenates tensors along a specified dimension. It is commonly used in artificial neural networks (ANNs) to combine the outputs of multiple layers or to concatenate multiple input tensors. Page 131 on PDF.\n",
        "\n",
        "✈ We can do this using view, which is a PyTorch method that changes the shape of a tensor without changing its contents. -1 is a special parameter to view that means \"make this axis as big as necessary to fit all the data\n",
        "\n",
        "Can you draw the neuronal network, which is manually trained in chapter \"The MNIST Loss Function\"?\n",
        "\n",
        "Why is it not possible to use the accuracy as loss function?\n",
        "\n",
        "✈ A very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function—if we do, most of the time our gradients will actually be 0, and the model will not be able to learn from that number.\n",
        "\n",
        "What is the defined `mnist_loss` function doing? \n",
        "\n",
        "✈ Takes the mean of the previous tensor\n",
        "\n",
        "```\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "```\n",
        "\n",
        "Why do we need additionaly the sigmoid() function? What is it technically in our TLU?\n",
        "\n",
        "Again, what are mini batches, why are we using them and why should they be shuffeld? \n",
        "\n",
        "So instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a mini-batch.  That is, you'd be going to the trouble of updating the weights, but taking into account only how that would improve the model's performance on that single item.\n",
        "\n",
        "Another good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. Rather than simply enumerating our dataset in order for every epoch, instead what we normally do is randomly shuffle it on every epoch, before we create mini-batches."
      ],
      "metadata": {
        "id": "iIBgQ5f43H6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Part\n",
        "\n",
        "Try to understand all parts of the code needed to manually train a single TLU/Perceptron, so use and copy all parts of the code from \"First Try: Pixel Similarity\" to the \"Putting it all together\" chapter. In the second step, use an optimizer, a second layer, and a ReLU as a hidden activation function to train a simple neural network. When copying the code, think carefully about what you really need and how you can summarize it as compactly as possible. (Probably each attempt requires about 15 lines of code.)"
      ],
      "metadata": {
        "id": "aoQq7z5D3XXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TxwyNuzj3DYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6bb7816-4773-4f74-bafe-86ea04bbcf68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [1 1], Output: 1\n",
            "Input: [ 1 -1], Output: -1\n",
            "Input: [-1  1], Output: -1\n",
            "Input: [-1 -1], Output: -1\n"
          ]
        }
      ],
      "source": [
        "#YOUR TASK: Manually train a single layer perceptron without using an optimizer.\n",
        "import numpy as np\n",
        "\n",
        "# Define the step function\n",
        "def step_function(x):\n",
        "    return np.where(x > 0, 1, -1)\n",
        "\n",
        "# Define the training function\n",
        "def train(X, y, num_epochs, learning_rate):\n",
        "    # Initialize the weights and bias\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    \n",
        "    # Perform the training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(X.shape[0]):\n",
        "            # Forward pass\n",
        "            z = np.dot(X[i], w) + b\n",
        "            a = step_function(z)\n",
        "            \n",
        "            # Backward pass\n",
        "            if a != y[i]:\n",
        "                w += learning_rate * y[i] * X[i]\n",
        "                b += learning_rate * y[i]\n",
        "    \n",
        "    # Return the weights and bias\n",
        "    return w, b\n",
        "\n",
        "# Define the main function\n",
        "if __name__ == '__main__':\n",
        "    # Define the training data\n",
        "    X = np.array([[1, 1], [1, -1], [-1, 1], [-1, -1]])\n",
        "    y = np.array([1, -1, -1, -1])\n",
        "    \n",
        "    # Train the perceptron\n",
        "    w, b = train(X, y, num_epochs=10, learning_rate=0.1)\n",
        "    \n",
        "    # Test the perceptron\n",
        "    for i in range(X.shape[0]):\n",
        "        z = np.dot(X[i], w) + b\n",
        "        a = step_function(z)\n",
        "        print('Input: {}, Output: {}'.format(X[i], a))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR TASK: Train a simple two-layer neural network (two perceptrons + hidden activation function) with built-in functions and an optimizer.\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64) # input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(64, 10) # hidden layer to output layer\n",
        "        self.relu = nn.ReLU() # activation function\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1) # flatten the input tensor\n",
        "        x = self.relu(self.fc1(x)) # pass through hidden layer with ReLU activation function\n",
        "        x = self.fc2(x) # pass through output layer\n",
        "        return x\n",
        "\n",
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(Net().parameters(), lr=0.1)\n",
        "\n",
        "# Train the network\n",
        "net = Net()\n",
        "for epoch in range(5): # number of epochs\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader: # loop through mini-batches\n",
        "        optimizer.zero_grad() # zero the gradients\n",
        "        output = net(images) # forward pass\n",
        "        loss = criterion(output, labels) # calculate loss\n",
        "        loss.backward() # backward pass\n",
        "        optimizer.step() # update weights\n",
        "        running_loss += loss.item() # calculate running loss\n",
        "        \n",
        "    print(f\"Epoch {epoch+1} - Training loss: {running_loss/len(trainloader)}\")"
      ],
      "metadata": {
        "id": "UGsLRFtMbyRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143ffa90-a0d0-4272-bf78-a6d7eedb310c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Training loss: 2.3262079646592455\n",
            "Epoch 2 - Training loss: 2.3262049180866557\n",
            "Epoch 3 - Training loss: 2.3262290575865237\n",
            "Epoch 4 - Training loss: 2.326196160651982\n",
            "Epoch 5 - Training loss: 2.32622232899737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rf3BM0Q56Cmo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
