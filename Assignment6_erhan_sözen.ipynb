{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbgrGhtVKRga28GJyU+5b6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erhansozen/Assignments-LearningPortfolio/blob/main/Assignment6_erhan_s%C3%B6zen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical part (Assignment, May 31)\n",
        "The following task is a binary classification task. The first column is our target. All variables are categorical variables from which you have to create dummy variables. The target column has only the property Q or F, which is to be modeled. As always, please create a train and a test data set, e.g. 80:20 or 90:10. Your goal is to create a neural network that best predicts the target column. Use probabilities with the sigmoid function as discussed in the theory. When designing your neural network, play with a few different numbers and sizes of layers and different activation functions.\n",
        "\n",
        "To validate your model, you can compare your results with a logistic regression model. (Note: This data set should allow you to achieve very high accuracies: > 98% maybe even >99%) You can see how these results vary when you use 50% for testing and 50% for training."
      ],
      "metadata": {
        "id": "SO-4o9mu4dwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Ass6/data_homework_6.csv')\n",
        "# Step 1: Load and preprocess the data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Ass6/data_homework_6.csv\")  # Replace \"your_dataset.csv\" with the actual filename/path\n",
        "\n",
        "# Create dummy variables using one-hot encoding\n",
        "data_encoded = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "print(data.head(5))\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = data_encoded.iloc[:, 1:]\n",
        "y = data_encoded.iloc[:, 0]\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Design the neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))  # Adjust the number of neurons and activation function as needed\n",
        "model.add(Dense(8, activation='relu'))  # Adjust the number of neurons and activation function as needed\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Step 4: Train the neural network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)  # Adjust the number of epochs and batch size as needed\n",
        "\n",
        "# Step 5: Evaluate the neural network\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Step 6: Experiment with different architectures\n",
        "\n",
        "# Example 1: Adding more hidden layers\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))  # Additional hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Example 2: Trying different activation functions\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='tanh', input_dim=X_train.shape[1]))  # tanh activation function\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Example 3: Adjusting the number of neurons\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_dim=X_train.shape[1]))  # Increased number of neurons\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9uh8D3L2W28",
        "outputId": "468c2a7c-dea1-40a8-c6ad-70e6f13a085e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "  target variable_1 variable_2 variable_3 variable_4 variable_5 variable_6  \\\n",
            "0      Q          Y          T          O          U          Q          G   \n",
            "1      F          Y          T          Z          U          B          G   \n",
            "2      F          C          T          X          U          M          G   \n",
            "3      Q          Y          Z          X          U          Q          G   \n",
            "4      F          Y          T          H          G          O          G   \n",
            "\n",
            "  variable_7 variable_8 variable_9  ... variable_13 variable_14 variable_15  \\\n",
            "0          D          O          L  ...           T           X           X   \n",
            "1          D          C          L  ...           T           X           X   \n",
            "2          D          C          O  ...           T           X           X   \n",
            "3          D          O          O  ...           T           X           X   \n",
            "4          X          C          L  ...           T           X           X   \n",
            "\n",
            "  variable_16 variable_17 variable_18 variable_19 variable_20 variable_21  \\\n",
            "0           Q           X           P           Q           L           T   \n",
            "1           Q           X           P           Q           O           O   \n",
            "2           Q           X           P           Q           O           O   \n",
            "3           Q           X           P           Q           L           T   \n",
            "4           Q           X           P           F           O           B   \n",
            "\n",
            "  variable_22  \n",
            "0           V  \n",
            "1           H  \n",
            "2           N  \n",
            "3           V  \n",
            "4           H  \n",
            "\n",
            "[5 rows x 23 columns]\n",
            "Epoch 1/10\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 0.3675 - accuracy: 0.8866\n",
            "Epoch 2/10\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 0.0545 - accuracy: 0.9908\n",
            "Epoch 3/10\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9985\n",
            "Epoch 4/10\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9992\n",
            "Epoch 5/10\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.3599e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 6.3529e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.7761e-04 - accuracy: 1.0000\n",
            "51/51 [==============================] - 0s 1ms/step - loss: 4.7917e-04 - accuracy: 1.0000\n",
            "Test Loss: 0.0004791744577232748\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**use 50% for testing and 50% for training.**"
      ],
      "metadata": {
        "id": "4m9kdT344xvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (X) and target (y)\n",
        "X = data_encoded.iloc[:, 1:]\n",
        "y = data_encoded.iloc[:, 0]\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 3: Design the neural network architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))  # Adjust the number of neurons and activation function as needed\n",
        "model.add(Dense(8, activation='relu'))  # Adjust the number of neurons and activation function as needed\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Step 4: Train the neural network\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)  # Adjust the number of epochs and batch size as needed\n",
        "\n",
        "# Step 5: Evaluate the neural network\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "# Step 6: Experiment with different architectures\n",
        "\n",
        "# Example 1: Adding more hidden layers\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))  # Additional hidden layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Example 2: Trying different activation functions\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation='tanh', input_dim=X_train.shape[1]))  # tanh activation function\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Example 3: Adjusting the number of neurons\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_dim=X_train.shape[1]))  # Increased number of neurons\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoV1ct_V2uoH",
        "outputId": "8459ee84-1501-4c6a-8be5-665b7188df05"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "127/127 [==============================] - 1s 3ms/step - loss: 0.4783 - accuracy: 0.8016\n",
            "Epoch 2/10\n",
            "127/127 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.9727\n",
            "Epoch 3/10\n",
            "127/127 [==============================] - 0s 2ms/step - loss: 0.0410 - accuracy: 0.9941\n",
            "Epoch 4/10\n",
            "127/127 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.9970\n",
            "Epoch 5/10\n",
            "127/127 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9990\n",
            "Epoch 6/10\n",
            "127/127 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9990\n",
            "Epoch 7/10\n",
            "127/127 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 0.9990\n",
            "Epoch 8/10\n",
            "127/127 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9998\n",
            "Epoch 9/10\n",
            "127/127 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "127/127 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "127/127 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Test Loss: 0.0022909638937562704\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}