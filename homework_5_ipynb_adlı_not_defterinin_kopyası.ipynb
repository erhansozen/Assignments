{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erhansozen/Assignments-LearningPortfolio/blob/main/homework_5_ipynb_adl%C4%B1_not_defterinin_kopyas%C4%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following assignment consists of a theoretical part (learning portfolio) and a practical part (assignment). The goal is to build a classification model that predicts from which subject area a certain abstract originates. The plan would be that next week we will discuss your learnings from the theory part, that means you are relatively free to fill your Learning Portfolio on this new topic and in two weeks we will discuss your solutions of the Classification Model.\n",
        "\n",
        "#Theory part (filling your Learning Portfolio, May 10)\n",
        "\n",
        "In preparation for the practical part, I ask you to familiarize yourself with the following resources in the next week:\n",
        "\n",
        "1) Please watch the following video:\n",
        "\n",
        "https://course.fast.ai/Lessons/lesson4.html\n",
        "\n",
        "You are also welcome to watch the accompanying Kaggle notebook if you like the video.\n",
        "\n",
        "2) In addition to the video, I recommend you to read the first chapters of the course\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "\n",
        "\n",
        "Try to understand principle processes and log them in your learning portfolio! A few suggestions: What is a pre-trained NLP model? How do I load them? What is tokenization? What does fine-tuning mean? What types of NLP Models are there? What possibilities do I have with the Transformers package? etc...\n",
        "\n",
        "**What is a pre-trained NLP model?**\n",
        "✈ A pre-trained natural language processing (NLP) model is a model that has already been trained on a large amount of text data, usually using a deep learning algorithm such as neural networks. Pre-trained NLP models can be used to improve the accuracy and efficiency of NLP tasks without the need to train a model from scratch, which can be time-consuming and resource-intensive. \n",
        "\n",
        "**How do I load them?**\n",
        "✈ Loading a pre-trained NLP model depends on the specific library and framework being used. Here's a general overview of the steps involved in loading a pre-trained NLP model:\n",
        "\n",
        "Install the required dependencies: You'll need to make sure that you have the necessary software installed on your machine. For example, if you're using Python, you might need to install packages like TensorFlow, PyTorch, or Hugging Face transformers.\n",
        "\n",
        "Download the pre-trained model: You'll need to download the pre-trained model files for the specific NLP task you're working on. You can usually find these models on the website of the library or framework you're using.\n",
        "\n",
        "Load the model: Once you have the pre-trained model files, you'll need to load the model into memory using the appropriate library function. For example, if you're using TensorFlow, you might use the tf.saved_model.load() function to load a pre-trained model.\n",
        "\n",
        "Configure the model: Depending on the library and task, you might need to configure the pre-trained model by setting parameters or passing in additional information.\n",
        "\n",
        "Use the model: Once the pre-trained model is loaded and configured, you can use it to perform the desired NLP task. For example, if you're using a pre-trained model for sentiment analysis, you might pass in a piece of text and get back a score indicating the sentiment of the text.\n",
        "\n",
        "**What is tokenization?**\n",
        "✈ Tokenization is the process of breaking down a text document or a sentence into smaller units called tokens. In natural language processing (NLP), tokens are usually words, but they can also be other units such as subwords or characters.\n",
        "\n",
        "The tokenization process is an important step in NLP because it creates a structured representation of the text that can be used for analysis and modeling. Tokens provide a way to represent the information in a text document in a form that can be easily processed by machine learning algorithms.\n",
        "\n",
        "What does fine-tuning mean?\n",
        "✈ Fine-tuning is a technique in machine learning that involves taking a pre-trained model and adapting it to a new task or domain. In the context of natural language processing (NLP), fine-tuning is a common approach to adapting pre-trained language models for specific NLP tasks.\n",
        "\n",
        "Fine-tuning can be a powerful technique because it allows pre-trained models to be adapted for a wide range of tasks without requiring large amounts of labeled data or extensive training. However, it is important to use caution when fine-tuning pre-trained models, as overfitting can occur if the model is fine-tuned on a very small dataset or if the task is very different from the pre-training task.\n",
        "\n",
        "What types of NLP Models are there?\n",
        "✈ There are several types of NLP models used in natural language processing. Some of the most common types include:\n",
        "\n",
        "Language Models: Language models are used to predict the likelihood of a sequence of words. They are trained on large amounts of text data and can be used for tasks like machine translation, speech recognition, and text generation.\n",
        "\n",
        "Text Classification Models: Text classification models are used to classify text into categories. They are trained on labeled examples and can be used for tasks like sentiment analysis, spam detection, and topic classification.\n",
        "\n",
        "Named Entity Recognition (NER) Models: NER models are used to extract named entities from text, such as people, places, and organizations. They are trained on labeled examples and can be used for tasks like information extraction and document classification.\n",
        "\n",
        "Part-of-Speech (POS) Tagging Models: POS tagging models are used to label words in a sentence with their part of speech, such as noun, verb, or adjective. They are trained on labeled examples and can be used for tasks like text analysis and machine translation.\n",
        "\n",
        "Dependency Parsing Models: Dependency parsing models are used to identify the relationships between words in a sentence. They can be used for tasks like text analysis, machine translation, and information extraction.\n",
        "\n",
        "Question-Answering Models: Question-answering models are used to answer questions based on text. They are trained on labeled examples and can be used for tasks like information retrieval and chatbots.\n",
        "\n",
        "Text Summarization Models: Text summarization models are used to generate a summary of a longer text. They can be trained on labeled examples or unsupervised methods like clustering or clustering and can be used for tasks like news summarization or document summarization.\n",
        "\n",
        "What possibilities do I have with the Transformers package?\n",
        "✈ The Transformers package is a powerful toolkit for natural language processing (NLP) that provides a wide range of functionalities and possibilities. Some of the main possibilities you have with the Transformers package are:\n",
        "\n",
        "Pre-trained Models: The Transformers package provides access to a large number of pre-trained models for various NLP tasks. These models have been trained on massive amounts of data and can be fine-tuned for specific tasks with a few lines of code.\n",
        "\n",
        "Customizable Architectures: The Transformers package allows you to customize the architecture of pre-trained models, enabling you to fine-tune them for specific NLP tasks or to train your own models from scratch.\n",
        "\n",
        "Tokenization: The Transformers package provides a suite of tokenization functions that can be used to tokenize text into a format that can be processed by machine learning algorithms. It supports a wide range of languages and tokenization methods, including byte-pair encoding (BPE) and WordPiece.\n",
        "\n",
        "Training and Fine-Tuning: The Transformers package provides tools for training and fine-tuning models on specific NLP tasks. It supports both supervised and unsupervised learning, and provides a variety of training options, such as batch training, gradient accumulation, and learning rate schedules.\n",
        "\n",
        "Evaluation and Testing: The Transformers package provides tools for evaluating the performance of NLP models on various tasks, including accuracy, precision, recall, and F1 score. It also provides tools for testing models on new data and for visualizing model outputs.\n",
        "\n",
        "Integration with Other Libraries: The Transformers package can be easily integrated with other popular NLP libraries like PyTorch and TensorFlow, enabling you to take advantage of the features and capabilities of these libraries.\n",
        "\n",
        "#Practical part (Assignment, May 17)\n",
        "\n",
        "1) Preprocessing: The data which I provide as zip in Olat must be processed first, that means we need a table which has the following form:\n",
        "\n",
        "Keywords | Title | Abstract | Research Field\n",
        "\n",
        "The research field is determined by the name of the file.\n",
        "\n",
        "2) We need a training dataset and a test dataset. My suggestion would be that for each research field we use the first 5700 lines for the training dataset and the last 300 lines for the test dataset. Please stick to this because then we can compare our models better!\n",
        "\n",
        "3) Please use a pre-trained model from huggingface to build a classification model that tries to predict the correct research field from the 26. Please calculate the accuracy and the overall accuracy for all research fields. If you solve this task in a group, you can also try different pre-trained models. In addition to the abstracts, you can also see if the model improves if you include keywords and titles.\n",
        "\n",
        "Some links, which can help you:\n",
        "\n",
        "https://huggingface.co/docs/transformers/training\n",
        "\n",
        "https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
        "\n",
        "One last request: Please always use PyTorch and not TensorFlow!"
      ],
      "metadata": {
        "id": "Rv37EvemaCce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUi9az-cZ9zq"
      },
      "outputs": [],
      "source": [
        "#YOUR TASK"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Addition: Accuracy measures whether the research field with the highest probability value matches the target. With 26 research fields, it would also be interesting to know if the correct target is at least among the three highest probability values.\n",
        "\n",
        "$\\begin{pmatrix} A\\\\ B \\\\ C \\\\D \\\\E \\end{pmatrix} = \\begin{pmatrix} 0.1\\\\ 0.95 \\\\ 0.5 \\\\0.2 \\\\0.3 \\end{pmatrix} → \\text{Choice}_1 = B, \\text{Choice}_3 = B,C,E$"
      ],
      "metadata": {
        "id": "Up3aCw__4f4d"
      }
    }
  ]
}